{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8abb3871-22cf-4bf2-b81d-9a163e586340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE santig_120781.ingesta_nueva_bronze.orders \n",
    "SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01811c16-1ef1-441e-b167-46b8d5fcf3cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE santig_120781.ingesta_nueva_bronze.orders DROP COLUMN IF EXISTS _c7;\n",
    "ALTER TABLE santig_120781.ingesta_nueva_bronze.orders DROP COLUMN IF EXISTS _c8;\n",
    "ALTER TABLE santig_120781.ingesta_nueva_bronze.orders DROP COLUMN IF EXISTS _c9;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9c4beb-b211-48ae-a7e8-9f7c53c7be84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>order_id</td><td>int</td><td>null</td></tr><tr><td>user_id</td><td>int</td><td>null</td></tr><tr><td>eval_set</td><td>string</td><td>null</td></tr><tr><td>order_number</td><td>int</td><td>null</td></tr><tr><td>order_dow</td><td>int</td><td>null</td></tr><tr><td>order_hour_of_day</td><td>int</td><td>null</td></tr><tr><td>days_since_prior_order</td><td>double</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "order_id",
         "int",
         null
        ],
        [
         "user_id",
         "int",
         null
        ],
        [
         "eval_set",
         "string",
         null
        ],
        [
         "order_number",
         "int",
         null
        ],
        [
         "order_dow",
         "int",
         null
        ],
        [
         "order_hour_of_day",
         "int",
         null
        ],
        [
         "days_since_prior_order",
         "double",
         null
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "col_name",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "data_type",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "comment",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 5
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql DESCRIBE TABLE santig_120781.ingesta_nueva_bronze.orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f97344b5-0de5-4cc2-ae0c-b5844fa112cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACION\n",
    "# ============================================================\n",
    "path_ingesta = \"abfss://raw@adlsmardata1307.dfs.core.windows.net/ingesta_nueva/\"\n",
    "CATALOGO     = \"santig_120781\"\n",
    "ESQUEMA      = f\"{CATALOGO}.ingesta_nueva_bronze\"\n",
    "\n",
    "# Columnas exactas que debe tener cada tabla Bronze (sin columnas basura)\n",
    "columnas_bronze = {\n",
    "    \"orders\": [\"order_id\", \"user_id\", \"eval_set\", \"order_number\",\n",
    "               \"order_dow\", \"order_hour_of_day\", \"days_since_prior_order\", \"_process_date\"],\n",
    "    \"products\": [\"product_id\", \"product_name\", \"aisle_id\", \"department_id\", \"_process_date\"],\n",
    "    \"departments\": [\"department_id\", \"department\", \"_process_date\"],\n",
    "    \"aisles\": [\"aisle_id\", \"aisle\", \"_process_date\"],\n",
    "    \"order_products__prior\": [\"order_id\", \"product_id\", \"add_to_cart_order\", \"reordered\", \"_process_date\"],\n",
    "    \"order_products__train\": [\"order_id\", \"product_id\", \"add_to_cart_order\", \"reordered\", \"_process_date\"],\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# PROCESO DE INGESTA\n",
    "# ============================================================\n",
    "try:\n",
    "    files = dbutils.fs.ls(path_ingesta)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"No se pudo acceder a la ruta de ingesta: {path_ingesta}\\nDetalle: {e}\")\n",
    "\n",
    "archivos_procesados = 0\n",
    "archivos_con_error  = 0\n",
    "\n",
    "for file in files:\n",
    "    if not file.name.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    table_name = file.name.split('_')[0].replace(\".csv\", \"\")\n",
    "    full_table = f\"{ESQUEMA}.{table_name}\"\n",
    "\n",
    "    print(f\"\\nProcesando: {file.name}  ->  {full_table}\")\n",
    "\n",
    "    try:\n",
    "        # 1. Lectura del CSV\n",
    "        df_nuevo = spark.read \\\n",
    "            .option(\"header\", \"True\") \\\n",
    "            .option(\"inferSchema\", \"True\") \\\n",
    "            .option(\"sep\", \";\") \\\n",
    "            .csv(file.path)\n",
    "\n",
    "        # 2. Agregar columna de auditoria\n",
    "        df_nuevo = df_nuevo.withColumn(\"_process_date\", F.current_timestamp())\n",
    "\n",
    "        # 3. Seleccionar SOLO las columnas validas (elimina _c7, _c8, _c9, etc.)\n",
    "        columnas_validas = columnas_bronze.get(table_name)\n",
    "\n",
    "        if columnas_validas is None:\n",
    "            print(f\"  AVISO: Tabla '{table_name}' no esta en el mapa de columnas. Se usaran todas las columnas del CSV.\")\n",
    "            df_para_guardar = df_nuevo\n",
    "        else:\n",
    "            columnas_disponibles = [c for c in columnas_validas if c in df_nuevo.columns]\n",
    "            columnas_faltantes   = [c for c in columnas_validas if c not in df_nuevo.columns]\n",
    "\n",
    "            if columnas_faltantes:\n",
    "                print(f\"  AVISO: Columnas faltantes en el CSV (se ignoraran): {columnas_faltantes}\")\n",
    "\n",
    "            df_para_guardar = df_nuevo.select(*columnas_disponibles)\n",
    "\n",
    "        # 4. Verificar que el DataFrame no esta vacio\n",
    "        if df_para_guardar.limit(1).count() == 0:\n",
    "            print(f\"  AVISO: El archivo {file.name} esta vacio. Se omite.\")\n",
    "            continue\n",
    "\n",
    "        # 5. Escritura: overwrite si la tabla no existe, append si ya existe\n",
    "        tabla_existe = spark.catalog.tableExists(full_table)\n",
    "\n",
    "        if not tabla_existe:\n",
    "            df_para_guardar.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(full_table)\n",
    "            print(f\"  OK: Tabla creada y datos cargados.\")\n",
    "        else:\n",
    "            # Validar esquema antes del append para evitar el AnalysisException\n",
    "            esquema_tabla = set([f.name for f in spark.read.table(full_table).schema.fields])\n",
    "            esquema_df    = set([f.name for f in df_para_guardar.schema.fields])\n",
    "\n",
    "            if esquema_tabla != esquema_df:\n",
    "                diferencias = esquema_tabla.symmetric_difference(esquema_df)\n",
    "                raise Exception(\n",
    "                    f\"Mismatch de esquema detectado antes de escribir.\\n\"\n",
    "                    f\"  Columnas en tabla:     {sorted(esquema_tabla)}\\n\"\n",
    "                    f\"  Columnas en DataFrame: {sorted(esquema_df)}\\n\"\n",
    "                    f\"  Diferencias:           {diferencias}\\n\"\n",
    "                    f\"  Ejecuta el ALTER TABLE para limpiar la tabla y vuelve a correr.\"\n",
    "                )\n",
    "\n",
    "            df_para_guardar.write.format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .saveAsTable(full_table)\n",
    "            print(f\"  OK: Datos aÃ±adidos en modo append.\")\n",
    "\n",
    "        archivos_procesados += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR procesando {file.name}: {e}\")\n",
    "        archivos_con_error += 1\n",
    "\n",
    "# ============================================================\n",
    "# RESUMEN\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"RESUMEN DE INGESTA\")\n",
    "print(f\"  Archivos procesados exitosamente : {archivos_procesados}\")\n",
    "print(f\"  Archivos con error               : {archivos_con_error}\")\n",
    "print(f\"{'='*55}\")\n",
    "\n",
    "# ============================================================\n",
    "# LIMPIEZA DE CARPETA (solo si todo fue exitoso)\n",
    "# ============================================================\n",
    "if archivos_con_error == 0 and archivos_procesados > 0:\n",
    "    dbutils.fs.rm(path_ingesta, recurse=True)\n",
    "    dbutils.fs.mkdirs(path_ingesta)\n",
    "    print(\"Carpeta de ingesta limpiada correctamente.\")\n",
    "elif archivos_con_error > 0:\n",
    "    print(\"No se limpio la carpeta porque hubo errores. Revisa los archivos fallidos.\")\n",
    "else:\n",
    "    print(\"No habia archivos CSV para procesar.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8610938651720514,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesta_Incremental_Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}